{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2b790-9d09-4abd-bfb5-dd101bd956b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50291e16-7c20-4404-aebd-f0866f1359df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e14bbf-b477-478b-b6f7-294c96d99afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af6511-6d07-48bd-92fc-61a724bc243d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7bdcf07-1db3-4f31-aab2-ad74adb06606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - imports & config\n",
    "import os, io, json, re, math\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import fitz                # PyMuPDF\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector DB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Ollama (local LLM client)\n",
    "import ollama\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "POPLER_PATH = r\"C:\\Users\\lenovo\\Downloads\\Release-25.12.0-0\\poppler-25.12.0\\Library\\bin\"   # <-- change to your poppler bin (Windows)\n",
    "TESSERACT_CMD = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"  # <-- change if needed\n",
    "pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-mpnet-base-v2\"   # small CPU embedding model\n",
    "EMBED_BATCH = 64\n",
    "\n",
    "# Ollama models (change if you pulled different ones)\n",
    "CLASSIFIER_MODEL = \"mistral-nemo:12b\"\n",
    "EXTRACTION_MODEL = \"mistral-nemo:12b\"\n",
    "\n",
    "# Chroma persistent directory\n",
    "CHROMA_DIR = \"chroma_store\"\n",
    "\n",
    "# Chunk size for text chunks used to create embeddings\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "302d556e-48de-4a85-b703-23d585c4d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - text extraction (auto-detect)\n",
    "def extract_text_auto(pdf_path: str, ocr_dpi=300, ocr_lang=\"eng\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns: list of page texts (one element per page).\n",
    "    Uses PyMuPDF for machine text and falls back to OCR per-page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_text = []\n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        try:\n",
    "            page_text = page.get_text().strip()\n",
    "        except Exception:\n",
    "            page_text = \"\"\n",
    "        if page_text and len(page_text) > 40:\n",
    "            pages_text.append(page_text)\n",
    "        else:\n",
    "            # Try PyMuPDF pixmap -> PIL image -> pytesseract\n",
    "            try:\n",
    "                pix = page.get_pixmap(dpi=ocr_dpi)\n",
    "                img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "            except Exception:\n",
    "                # fallback to pdf2image (slower)\n",
    "                images = convert_from_path(pdf_path, dpi=ocr_dpi, poppler_path=POPLER_PATH)\n",
    "                img = images[i-1] if i-1 < len(images) else images[0]\n",
    "            ocr_text = pytesseract.image_to_string(img, lang=ocr_lang)\n",
    "            pages_text.append(ocr_text)\n",
    "    return pages_text\n",
    "\n",
    "def join_pages(pages: List[str]) -> str:\n",
    "    txt = \"\\n\".join(pages)\n",
    "    return \" \".join(txt.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9429faea-769f-47c4-be27-b1fb726d5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - chunking long text into overlapping chunks\n",
    "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    text = text.strip()\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b1067df-ce06-4e73-ae8d-a79067e96d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - load SentenceTransformer once\n",
    "print(\"Loading embedding model:\", EMBED_MODEL_NAME)\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfd2973c-5385-4345-b135-afa5d5968c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extraction_prompt(category, text):\n",
    "    \"\"\"\n",
    "    Returns a strict JSON extraction prompt for the given document category.\n",
    "    Bank Statements and Salary Slips have been intentionally removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) HOSPITAL INSURANCE CLAIM FORM\n",
    "    # ============================================================\n",
    "    if category == \"Claim Document\":\n",
    "        return f\"\"\"\n",
    "You are an expert in extracting structured fields from HOSPITAL INSURANCE CLAIM FORMS.\n",
    "Extract ONLY values explicitly visible — NO hallucination.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Missing → return \"Not mentioned\".\n",
    "2. Preserve original formatting of dates, amounts, and labels.\n",
    "3. Checkboxes: ✔ / ✓ / X / Yes / Selected = mark as present.\n",
    "4. Keep OCR errors minimal — extract only clear text.\n",
    "5. AI Summary must be max 2 sentences.\n",
    "\n",
    "FIELDS TO EXTRACT:\n",
    "- Primary Insured Name\n",
    "- Policy Number\n",
    "- TPA / Company ID\n",
    "- Employee / Member ID\n",
    "- Patient Name\n",
    "- Insurance Company Name\n",
    "- Hospital Name\n",
    "- Hospital Type\n",
    "- Admission Date\n",
    "- Discharge Date\n",
    "- Injury / Illness Type\n",
    "- Claim Type\n",
    "- Claim Amount\n",
    "- Billing Breakdown:\n",
    "    - Pre Hospitalization\n",
    "    - Hospitalization\n",
    "    - Post Hospitalization\n",
    "    - Pharmacy Bills\n",
    "    - Ambulance Charges\n",
    "    - Other Charges\n",
    "- Submitted Documents Checklist (list)\n",
    "- City / Location\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    "  \"Primary Insured Name\": \"\",\n",
    "  \"Policy Number\": \"\",\n",
    "  \"TPA / Company ID\": \"\",\n",
    "  \"Employee / Member ID\": \"\",\n",
    "  \"Patient Name\": \"\",\n",
    "  \"Insurance Company Name\": \"\",\n",
    "  \"Hospital Name\": \"\",\n",
    "  \"Hospital Type\": \"\",\n",
    "  \"Admission Date\": \"\",\n",
    "  \"Discharge Date\": \"\",\n",
    "  \"Injury / Illness Type\": \"\",\n",
    "  \"Claim Type\": \"\",\n",
    "  \"Claim Amount\": \"\",\n",
    "  \"Billing Breakdown\": {{\n",
    "    \"Pre Hospitalization\": \"\",\n",
    "    \"Hospitalization\": \"\",\n",
    "    \"Post Hospitalization\": \"\",\n",
    "    \"Pharmacy Bills\": \"\",\n",
    "    \"Ambulance Charges\": \"\",\n",
    "    \"Other Charges\": \"\"\n",
    "  }},\n",
    "  \"Submitted Documents Checklist\": [],\n",
    "  \"City / Location\": \"\",\n",
    "  \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) HEALTH INSURANCE POLICY\n",
    "    # ============================================================\n",
    "    if category == \"Health Insurance Policy\":\n",
    "        return f\"\"\"\n",
    "You are an expert in HEALTH INSURANCE POLICY extraction.\n",
    "\n",
    "RULES:\n",
    "- Extract ONLY explicit text from document.\n",
    "- Missing → \"Not mentioned\".\n",
    "- Preserve formatting.\n",
    "\n",
    "FIELDS:\n",
    "- Policy Holder Name\n",
    "- Policy Number\n",
    "- Insurance Company\n",
    "- TPA Name\n",
    "- Sum Insured\n",
    "- Coverage Type\n",
    "- Policy Start Date\n",
    "- Policy End Date\n",
    "- UIN / Product Code\n",
    "- AI Summary (max 2 sentences)\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Policy Holder Name\": \"\",\n",
    " \"Policy Number\": \"\",\n",
    " \"Insurance Company\": \"\",\n",
    " \"TPA Name\": \"\",\n",
    " \"Sum Insured\": \"\",\n",
    " \"Coverage Type\": \"\",\n",
    " \"Policy Start Date\": \"\",\n",
    " \"Policy End Date\": \"\",\n",
    " \"UIN / Product Code\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) MOTOR INSURANCE POLICY\n",
    "    # ============================================================\n",
    "    if category == \"Motor Insurance Policy\":\n",
    "        return f\"\"\"\n",
    "You extract structured fields from MOTOR INSURANCE POLICY documents.\n",
    "\n",
    "RULES:\n",
    "- No hallucination.\n",
    "- Preserve original formatting.\n",
    "\n",
    "FIELDS:\n",
    "- Policy Holder Name\n",
    "- Policy Number\n",
    "- Insurance Company\n",
    "- Coverage Type\n",
    "- Vehicle Model\n",
    "- Registration Number\n",
    "- Engine / Chassis Number\n",
    "- Policy Start Date\n",
    "- Policy End Date\n",
    "- IDV (if present)\n",
    "- UIN\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Policy Holder Name\": \"\",\n",
    " \"Policy Number\": \"\",\n",
    " \"Insurance Company\": \"\",\n",
    " \"Coverage Type\": \"\",\n",
    " \"Vehicle Model\": \"\",\n",
    " \"Registration Number\": \"\",\n",
    " \"Engine / Chassis Number\": \"\",\n",
    " \"Policy Start Date\": \"\",\n",
    " \"Policy End Date\": \"\",\n",
    " \"IDV\": \"\",\n",
    " \"UIN\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) LIFE INSURANCE POLICY\n",
    "    # ============================================================\n",
    "    if category == \"Life Insurance Policy\":\n",
    "        return f\"\"\"\n",
    "You extract structured fields from LIFE INSURANCE POLICY documents.\n",
    "\n",
    "RULES:\n",
    "- No guessing. Missing → \"Not mentioned\".\n",
    "- Preserve formatting.\n",
    "- AI Summary: max 2 sentences.\n",
    "\n",
    "FIELDS:\n",
    "- Policy Holder Name\n",
    "- Insured Person / Life Assured\n",
    "- Policy Number\n",
    "- Insurance Company\n",
    "- Plan / Policy Type\n",
    "- Sum Assured\n",
    "- Annual / Monthly Premium\n",
    "- Policy Term\n",
    "- Premium Paying Term\n",
    "- Nominee Name\n",
    "- Policy Start Date\n",
    "- Policy End Date / Maturity Date\n",
    "- UIN / Product Code\n",
    "- Benefit Type\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Policy Holder Name\": \"\",\n",
    " \"Insured Person / Life Assured\": \"\",\n",
    " \"Policy Number\": \"\",\n",
    " \"Insurance Company\": \"\",\n",
    " \"Plan / Policy Type\": \"\",\n",
    " \"Sum Assured\": \"\",\n",
    " \"Annual / Monthly Premium\": \"\",\n",
    " \"Policy Term\": \"\",\n",
    " \"Premium Paying Term\": \"\",\n",
    " \"Nominee Name\": \"\",\n",
    " \"Policy Start Date\": \"\",\n",
    " \"Policy End Date / Maturity Date\": \"\",\n",
    " \"UIN / Product Code\": \"\",\n",
    " \"Benefit Type\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) IRCTC TRAIN TICKET\n",
    "    # ============================================================\n",
    "    if category == \"IRCTC Ticket\":\n",
    "        return f\"\"\"\n",
    "You extract structured fields from IRCTC train tickets.\n",
    "\n",
    "FIELDS:\n",
    "- Passenger Name\n",
    "- Train Number\n",
    "- Train Name\n",
    "- Date of Journey\n",
    "- Boarding Station\n",
    "- Destination Station\n",
    "- Class\n",
    "- PNR\n",
    "- Booking Status\n",
    "- Current Status\n",
    "- Coach / Seat\n",
    "- Fare\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Passenger Name\": \"\",\n",
    " \"Train Number\": \"\",\n",
    " \"Train Name\": \"\",\n",
    " \"Date of Journey\": \"\",\n",
    " \"Boarding Station\": \"\",\n",
    " \"Destination Station\": \"\",\n",
    " \"Class\": \"\",\n",
    " \"PNR\": \"\",\n",
    " \"Booking Status\": \"\",\n",
    " \"Current Status\": \"\",\n",
    " \"Coach / Seat\": \"\",\n",
    " \"Fare\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 6) INVOICE\n",
    "    # ============================================================\n",
    "    if category == \"Invoice\":\n",
    "        return f\"\"\"\n",
    "You extract structured fields from INVOICE documents.\n",
    "\n",
    "FIELDS:\n",
    "- Invoice Number\n",
    "- Invoice Date\n",
    "- Vendor Name\n",
    "- Customer Name\n",
    "- Items (name, quantity, price, amount)\n",
    "- Tax Amount\n",
    "- Total Amount\n",
    "- Payment Terms\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Invoice Number\": \"\",\n",
    " \"Invoice Date\": \"\",\n",
    " \"Vendor Name\": \"\",\n",
    " \"Customer Name\": \"\",\n",
    " \"Items\": [],\n",
    " \"Tax Amount\": \"\",\n",
    " \"Total Amount\": \"\",\n",
    " \"Payment Terms\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) LEGAL NOTICE  ✅ FIXED & PROPERLY INDENTED\n",
    "    # ============================================================\n",
    "    if category == \"Legal Notice\":\n",
    "        return f\"\"\"\n",
    "You are an expert in extracting structured information from LEGAL NOTICE DOCUMENTS.\n",
    "\n",
    "RULES:\n",
    "- Extract ONLY what is explicitly written.\n",
    "- No hallucination. Missing → \"Not mentioned\".\n",
    "- AI Summary = 2 clear sentences.\n",
    "\n",
    "FIELDS:\n",
    "- Document Title\n",
    "- Client / Complainant\n",
    "- Accused / Respondent\n",
    "- Advocate\n",
    "- Issue Description\n",
    "- Contract Amount\n",
    "- Amount Pending\n",
    "- Payment Deadline\n",
    "- Interest Rate\n",
    "- Notice Date\n",
    "- Legal Actions Mentioned\n",
    "- AI Summary\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    "  \"Document Title\": \"\",\n",
    "  \"Client / Complainant\": \"\",\n",
    "  \"Accused / Respondent\": \"\",\n",
    "  \"Advocate\": \"\",\n",
    "  \"Issue Description\": \"\",\n",
    "  \"Contract Amount\": \"\",\n",
    "  \"Amount Pending\": \"\",\n",
    "  \"Payment Deadline\": \"\",\n",
    "  \"Interest Rate\": \"\",\n",
    "  \"Notice Date\": \"\",\n",
    "  \"Legal Actions Mentioned\": \"\",\n",
    "  \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # FALLBACK\n",
    "    # ============================================================\n",
    "    return f\"\"\"\n",
    "Extract ONLY explicit metadata from this document.\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return STRICT JSON ONLY:\n",
    "{{\n",
    " \"Document Type\": \"\",\n",
    " \"Key Fields\": \"\",\n",
    " \"AI Summary\": \"\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66bf1353-1360-4814-896a-55d31054b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this replace for your upsert_pdf_to_chroma function\n",
    "def upsert_pdf_to_chroma(pdf_path: str, collection_name: str, clear_existing=False):\n",
    "    \"\"\"\n",
    "    Extract text, chunk, embed, and upsert to Chroma collection.\n",
    "    - If clear_existing=True, will delete any existing items with same collection_name.\n",
    "    \"\"\"\n",
    "    pages = extract_text_auto(pdf_path)\n",
    "    full_text = \" \".join(pages)\n",
    "    chunks = chunk_text(full_text)\n",
    "\n",
    "    client = get_chroma_client()\n",
    "    # create or get collection\n",
    "    try:\n",
    "        col = client.get_collection(collection_name)\n",
    "    except Exception:\n",
    "        col = client.create_collection(collection_name)\n",
    "\n",
    "    ids = [f\"{os.path.basename(pdf_path)}:::chunk::{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"source\": pdf_path, \"chunk_index\": i} for i in range(len(chunks))]\n",
    "\n",
    "    # compute embeddings in batches and convert to plain python lists\n",
    "    embeddings = []\n",
    "    for i in range(0, len(chunks), EMBED_BATCH):\n",
    "        batch = chunks[i:i+EMBED_BATCH]\n",
    "        embs = embed_model.encode(batch, show_progress_bar=False, convert_to_numpy=True)\n",
    "        # convert each row to native python list for chroma safety\n",
    "        for row in embs:\n",
    "            embeddings.append(row.tolist() if hasattr(row, \"tolist\") else list(row))\n",
    "\n",
    "    if len(embeddings) != len(chunks):\n",
    "        raise RuntimeError(f\"Embeddings length mismatch: {len(embeddings)} vs chunks {len(chunks)}\")\n",
    "\n",
    "    # Optionally remove any previous chunks for this file to avoid duplicates\n",
    "    if clear_existing:\n",
    "        try:\n",
    "            # naive delete by ids if present\n",
    "            existing = col.get(include=[\"ids\", \"metadatas\"])\n",
    "            # collect ids that start with this filename\n",
    "            to_delete = [eid for eid in existing.get(\"ids\", []) if eid.startswith(os.path.basename(pdf_path) + \":::chunk::\")]\n",
    "            if to_delete:\n",
    "                col.delete(ids=to_delete)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Add (or upsert)\n",
    "    col.add(ids=ids, documents=chunks, metadatas=metadatas, embeddings=embeddings)\n",
    "    print(f\"Inserted {len(chunks)} chunks into collection '{collection_name}'\")\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5d1353-6bea-41c9-93a1-2c361d87e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - retrieval & simple embedding-based classifier\n",
    "def retrieve_similar_chunks(query: str, collection_name: str, top_k=5):\n",
    "    client = get_chroma_client()\n",
    "    col = client.get_collection(collection_name)\n",
    "\n",
    "    # FIXED embedding call\n",
    "    q_emb = embed_model([query])[0]\n",
    "    try:\n",
    "        q_emb = q_emb.tolist()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    results = col.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    metas = results.get(\"metadatas\", [[]])[0]\n",
    "    distances = results.get(\"distances\", [[]])[0] if \"distances\" in results else [None] * len(docs)\n",
    "\n",
    "    # Prevent None errors\n",
    "    safe_docs = [(doc if isinstance(doc, str) else \"\") for doc in docs]\n",
    "\n",
    "    return list(zip(safe_docs, metas, distances))\n",
    "\n",
    "\n",
    "# Small helper: quick category candidates by checking common category keywords inside top chunks\n",
    "# Small helper: quick category candidates by checking common category keywords inside top chunks\n",
    "CATEGORY_KEYWORDS = {\n",
    "    \"Claim Document\": [\n",
    "        \"claim\", \"admission\", \"discharge\", \"diagnosis\", \"tpa\",\n",
    "        \"claim amount\", \"hospital\", \"pre-authorization\"\n",
    "    ],\n",
    "\n",
    "    \"Health Insurance Policy\": [\n",
    "        \"policy\", \"sum insured\", \"uin\", \"health insurance\",\n",
    "        \"coverage\", \"policy schedule\"\n",
    "    ],\n",
    "\n",
    "    \"Motor Insurance Policy\": [\n",
    "        \"motor\", \"vehicle\", \"engine\", \"chassis\", \"registration\",\n",
    "        \"idv\", \"two wheeler\", \"four wheeler\"\n",
    "    ],\n",
    "\n",
    "    \"Life Insurance Policy\": [\n",
    "        \"life assured\", \"sum assured\", \"premium\", \"maturity\",\n",
    "        \"death benefit\", \"survival benefit\", \"policy term\"\n",
    "    ],\n",
    "\n",
    "    \"Hospital Bill\": [\n",
    "        \"bill\", \"invoice\", \"hospital charges\", \"room rent\",\n",
    "        \"doctor fee\", \"pharmacy\", \"investigation\"\n",
    "    ],\n",
    "\n",
    "    \"Payment Receipt\": [\n",
    "        \"receipt\", \"amount paid\", \"paid on\", \"transaction id\",\n",
    "        \"payment received\", \"cash received\"\n",
    "    ],\n",
    "\n",
    "    \"KYC / Identity Document\": [\n",
    "        \"passport\", \"aadhar\", \"aadhaar\", \"identity\", \"dob\",\n",
    "        \"pan\", \"voter id\", \"driving license\"\n",
    "    ],\n",
    "\n",
    "    \"IRCTC Ticket\": [\n",
    "        \"pnr\", \"train\", \"railway\", \"departure\", \"arrival\",\n",
    "        \"berth\", \"coach\", \"irctc\", \"journey\"\n",
    "    ],\n",
    "\n",
    "    \"Invoice\": [\n",
    "        \"invoice\", \"gst\", \"total payable\", \"unit price\", \"quantity\",\n",
    "        \"tax invoice\", \"hsn\", \"igst\", \"cgst\", \"sgst\"\n",
    "    ],\n",
    "\n",
    "    \"Legal Notice\": [\n",
    "        \"legal notice\", \"advocate\", \"lawyer\", \"contractor\",\n",
    "        \"agreement\", \"pending amount\", \"breach\", \"serve notice\",\n",
    "        \"defaulter\", \"obligation\", \"due amount\", \"legal action\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def embedding_candidate_category(chunks_with_meta):\n",
    "    scores = {}\n",
    "\n",
    "    for doc, meta, dist in chunks_with_meta:\n",
    "        if not doc:\n",
    "            continue\n",
    "\n",
    "        snippet = str(doc).lower()\n",
    "\n",
    "        for category, keywords in CATEGORY_KEYWORDS.items():\n",
    "            for k in keywords:\n",
    "                if k in snippet:\n",
    "                    scores[category] = scores.get(category, 0) + 1\n",
    "\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    valid_chunks = max(1, sum(1 for doc,_,_ in chunks_with_meta if doc))\n",
    "\n",
    "    for c in scores:\n",
    "        scores[c] /= valid_chunks\n",
    "\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b40e997-0b74-455d-ac9c-27a9d3209b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_classify_with_context(text: str, model=CLASSIFIER_MODEL, top_k_snippets: List[str] = None):\n",
    "    \"\"\"\n",
    "    Uses an LLM to classify the document into one category.\n",
    "    Matches EXACTLY the categories supported in get_extraction_prompt().\n",
    "    \"\"\"\n",
    "\n",
    "    ctx = \"\\n\\n\".join(top_k_snippets) if top_k_snippets else text[:3000]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a highly accurate professional document classifier.\n",
    "Classify the document into exactly ONE category from this list:\n",
    "\n",
    "- Claim Document\n",
    "- Health Insurance Policy\n",
    "- Motor Insurance Policy\n",
    "- Life Insurance Policy\n",
    "- Hospital Bill\n",
    "- Payment Receipt\n",
    "- KYC / Identity Document\n",
    "- IRCTC Ticket\n",
    "- Invoice\n",
    "- Legal Notice\n",
    "- Other\n",
    "\n",
    "RULES:\n",
    "1. Use ONLY explicit text visible in the context.\n",
    "2. If unsure or ambiguous → return \"Other\".\n",
    "3. Output ONLY valid JSON.\n",
    "4. Confidence MUST be between 0 and 1.\n",
    "\n",
    "Return JSON only in this exact format:\n",
    "{{ \"category\": \"\", \"confidence\": 0.0 }}\n",
    "\n",
    "CONTEXT:\n",
    "{ctx}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=model, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        content = resp[\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ ERROR: LLM classifier failed:\", e)\n",
    "        return {\"category\": \"Other\", \"confidence\": 0.0}\n",
    "\n",
    "    # strict json\n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "        if \"category\" in parsed and \"confidence\" in parsed:\n",
    "            return parsed\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    m = re.search(r\"\\{[\\s\\S]*?\\}\", content)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            if \"category\" in obj and \"confidence\" in obj:\n",
    "                return obj\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {\"category\": \"Other\", \"confidence\": 0.3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b3750d-98e6-4b86-a2f2-aa39f9aae28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_extraction_prompt(category: str, text: str) -> str:\\n    if category == \"Claim Document\":\\n        return f\"\"\"\\nYou are an expert at extracting structured fields from insurance CLAIM forms (health, motor).\\nExtract EXACT fields and RETURN ONLY JSON with the following keys:\\n{{ \\n \"Policy Holder Name\":\"\", \"Claimant Name\":\"\", \"Patient Name\":\"\",\\n \"Policy Number\":\"\", \"Insurance Company\":\"\", \"TPA Name\":\"\",\\n \"Hospital Name\":\"\", \"Admission Date\":\"\", \"Discharge Date\":\"\",\\n \"Diagnosis / Reason\":\"\", \"Claim Type\":\"\", \"Claim Amount\":\"\",\\n \"UIN\":\"\", \"Documents Submitted\": [], \"AI Summary\":\"\"\\n}}\\n\\nRules:\\n- Use \"Not mentioned\" for missing fields.\\n- Extract dates/amounts exactly as written.\\n- Documents Submitted should be a list of submitted/checked items found in text.\\n- AI Summary: 1-2 sentences stating claimant, policy no (if found), claim type, and amount (if found).\\n\\nContext Text:\\n{text}\\n\"\"\"\\n    if category == \"Health Insurance Policy\":\\n        return f\"\"\"\\n[Health policy extraction prompt omitted for brevity — implement similar strict JSON keys]\\n\"\"\"\\n    if category == \"Motor Insurance Policy\":\\n        return f\"\"\"\\n[Motor policy extraction prompt omitted for brevity — implement similar strict JSON keys]\\n\"\"\"\\n    # generic fallback:\\n    return f\"\"\"\\nExtract a short structured JSON summary from the document. Return ONLY JSON.\\nDocument:\\n{text}\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9 - improved prompt router (only key templates shown; add more as needed)\n",
    "'''def get_extraction_prompt(category: str, text: str) -> str:\n",
    "    if category == \"Claim Document\":\n",
    "        return f\"\"\"\n",
    "You are an expert at extracting structured fields from insurance CLAIM forms (health, motor).\n",
    "Extract EXACT fields and RETURN ONLY JSON with the following keys:\n",
    "{{ \n",
    " \"Policy Holder Name\":\"\", \"Claimant Name\":\"\", \"Patient Name\":\"\",\n",
    " \"Policy Number\":\"\", \"Insurance Company\":\"\", \"TPA Name\":\"\",\n",
    " \"Hospital Name\":\"\", \"Admission Date\":\"\", \"Discharge Date\":\"\",\n",
    " \"Diagnosis / Reason\":\"\", \"Claim Type\":\"\", \"Claim Amount\":\"\",\n",
    " \"UIN\":\"\", \"Documents Submitted\": [], \"AI Summary\":\"\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Use \"Not mentioned\" for missing fields.\n",
    "- Extract dates/amounts exactly as written.\n",
    "- Documents Submitted should be a list of submitted/checked items found in text.\n",
    "- AI Summary: 1-2 sentences stating claimant, policy no (if found), claim type, and amount (if found).\n",
    "\n",
    "Context Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "    if category == \"Health Insurance Policy\":\n",
    "        return f\"\"\"\n",
    "[Health policy extraction prompt omitted for brevity — implement similar strict JSON keys]\n",
    "\"\"\"\n",
    "    if category == \"Motor Insurance Policy\":\n",
    "        return f\"\"\"\n",
    "[Motor policy extraction prompt omitted for brevity — implement similar strict JSON keys]\n",
    "\"\"\"\n",
    "    # generic fallback:\n",
    "    return f\"\"\"\n",
    "Extract a short structured JSON summary from the document. Return ONLY JSON.\n",
    "Document:\n",
    "{text}\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277b2343-c9a5-47da-b0c9-031bf9c81bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields_via_llm(text: str, category: str, model=EXTRACTION_MODEL, max_chars=6000, rag_k=8, collection_name=None):\n",
    "    \"\"\"\n",
    "    Use RAG: retrieve top rag_k chunks and pass them as 'CONTEXT CHUNKS' to LLM extractor.\n",
    "    If collection_name is provided, we retrieve relevant chunks from Chroma.\n",
    "    \"\"\"\n",
    "    # If a collection is available, retrieve top chunks relevant to the doc head\n",
    "    context_block = \"\"\n",
    "    if collection_name:\n",
    "        # use the first 2000 chars as a query to retrieve relevant passages\n",
    "        q = text[:2000]\n",
    "        chunks = retrieve_similar_chunks(q, collection_name, top_k=rag_k)\n",
    "        snippets = [doc for doc,meta,d in chunks if doc]\n",
    "        if snippets:\n",
    "            context_block = \"\\n\\n---CONTEXT CHUNKS---\\n\\n\" + \"\\n\\n\".join(snippets)\n",
    "\n",
    "    # Build prompt using context_block + truncated text (fallback)\n",
    "    body_text = text[:max_chars]\n",
    "    prompt = get_extraction_prompt(category, body_text + (\"\\n\\n\" + context_block if context_block else \"\"))\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=model, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        content = resp[\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ ERROR: extraction LLM failed:\", e)\n",
    "        return {\"AI Summary\": \"\", \"Note\": f\"LLM call failed: {e}\"}\n",
    "\n",
    "    # remove triple backticks\n",
    "    content = re.sub(r\"```(?:json)?\", \"\", content).strip()\n",
    "\n",
    "    # try parse JSON robustly\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except:\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", content)\n",
    "        if m:\n",
    "            jtxt = m.group(0)\n",
    "            jtxt = re.sub(r\",\\s*}\", \"}\", jtxt)\n",
    "            jtxt = re.sub(r\",\\s*]\", \"]\", jtxt)\n",
    "            try:\n",
    "                return json.loads(jtxt)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return {\"AI Summary\": content[:2000], \"Note\": \"Failed to parse JSON\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b77b0ce-be17-4a03-8603-4c38ed1fbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - universal weighted confidence scorer\n",
    "def compute_confidence_from_dict(extracted: dict, category: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Computes confidence score dynamically based on category fields.\n",
    "    Prevents incorrect penalties for fields that are not applicable \n",
    "    to a given document type.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(extracted, dict) or len(extracted) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # ----------------------\n",
    "    # CATEGORY-SPECIFIC KEYS\n",
    "    # ----------------------\n",
    "\n",
    "    CATEGORY_HIGH = {\n",
    "        \"Claim Document\": {\n",
    "            \"Primary Insured Name\", \"Policy Number\", \"Patient Name\",\n",
    "            \"Hospital Name\", \"Admission Date\", \"Discharge Date\",\n",
    "            \"Claim Amount\", \"Claim Type\"\n",
    "        },\n",
    "        \"Health Insurance Policy\": {\n",
    "            \"Policy Holder Name\", \"Policy Number\", \"Insurance Company\",\n",
    "            \"Sum Insured\", \"Policy Start Date\", \"Policy End Date\"\n",
    "        },\n",
    "        \"Motor Insurance Policy\": {\n",
    "            \"Policy Holder Name\", \"Policy Number\", \"Registration Number\",\n",
    "            \"Vehicle Model\", \"Policy Start Date\", \"Policy End Date\"\n",
    "        },\n",
    "        \"Life Insurance Policy\": {\n",
    "            \"Policy Holder Name\", \"Insured Person / Life Assured\",\n",
    "            \"Policy Number\", \"Sum Assured\", \"Policy Term\"\n",
    "        },\n",
    "        \"IRCTC Ticket\": {\n",
    "            \"Passenger Name\", \"Train Number\", \"PNR\",\n",
    "            \"Date of Journey\", \"Boarding Station\", \"Destination Station\"\n",
    "        },\n",
    "        \"Invoice\": {\n",
    "            \"Invoice Number\", \"Vendor Name\", \"Customer Name\",\n",
    "            \"Invoice Date\", \"Total Amount\"\n",
    "        },\n",
    "        \"KYC / Identity Document\": {\n",
    "            \"Name\", \"DOB\", \"Document Number\"\n",
    "        },\n",
    "        \"Hospital Bill\": {\n",
    "            \"Hospital Name\", \"Total Amount\", \"Patient Name\"\n",
    "        },\n",
    "        \"Payment Receipt\": {\n",
    "            \"Receipt Number\", \"Amount Paid\", \"Date\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    CATEGORY_MEDIUM = {\n",
    "        \"Claim Document\": {\"TPA Name\", \"Injury / Illness Type\", \"Billing Breakdown\"},\n",
    "        \"Health Insurance Policy\": {\"Coverage Type\", \"TPA Name\"},\n",
    "        \"Motor Insurance Policy\": {\"Engine / Chassis Number\", \"Coverage Type\", \"IDV\"},\n",
    "        \"Life Insurance Policy\": {\"Premium Paying Term\", \"Nominee Name\"},\n",
    "        \"IRCTC Ticket\": {\"Booking Status\", \"Current Status\", \"Coach / Seat\"},\n",
    "        \"Invoice\": {\"Tax Amount\", \"Payment Terms\"},\n",
    "        \"KYC / Identity Document\": {\"Address\", \"Gender\"},\n",
    "        \"Hospital Bill\": {\"Room Rent\", \"Consultation Fee\"},\n",
    "        \"Payment Receipt\": {\"Payment Mode\", \"Reference Number\"},\n",
    "        \"PA Insurance Policy\": [\n",
    "    \"personal accident\", \"accidental death\", \"pa policy\",\n",
    "    \"permanent disability\", \"td\", \"ttd\", \"ppd\",\n",
    "    \"education grant\", \"accident cover\"\n",
    "]\n",
    "\n",
    "    }\n",
    "\n",
    "    SKIP = {\"AI Summary\", \"Note\"}\n",
    "\n",
    "    # Get relevant keys for category\n",
    "    high_keys = CATEGORY_HIGH.get(category, set())\n",
    "    med_keys  = CATEGORY_MEDIUM.get(category, set())\n",
    "\n",
    "    total_weight = 0\n",
    "    score = 0\n",
    "\n",
    "    def is_valid(v):\n",
    "        if v is None:\n",
    "            return False\n",
    "        s = str(v).strip()\n",
    "        if not s or s.lower() == \"not mentioned\":\n",
    "            return False\n",
    "        if s in {\"--\", \"-\", \"n/a\", \"xxx\", \"nil\"}:\n",
    "            return False\n",
    "        if len(s) == 1 and not s.isalnum():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # ----------------------\n",
    "    # SCORING LOOP\n",
    "    # ----------------------\n",
    "    for field, value in extracted.items():\n",
    "        if field in SKIP:\n",
    "            continue\n",
    "\n",
    "        # Detect nested dictionaries (Invoice items, Billing Breakdown, etc.)\n",
    "        if isinstance(value, dict):\n",
    "            # score nested fields individually\n",
    "            for k2, v2 in value.items():\n",
    "                w = 2 if k2 in med_keys else 1\n",
    "                total_weight += w\n",
    "                if is_valid(v2):\n",
    "                    score += w\n",
    "            continue\n",
    "\n",
    "        # Determine weight\n",
    "        if field in high_keys:\n",
    "            weight = 3\n",
    "        elif field in med_keys:\n",
    "            weight = 2\n",
    "        else:\n",
    "            weight = 1  # safe fallback for uncommon fields\n",
    "\n",
    "        total_weight += weight\n",
    "\n",
    "        if is_valid(value):\n",
    "            score += weight\n",
    "\n",
    "    if total_weight == 0:\n",
    "        return 0.5  # safe fallback\n",
    "\n",
    "    conf = score / total_weight\n",
    "    return round(min(max(conf, 0.0), 1.0), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b9c0c28-09ee-4f64-9201-3be8dec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CHROMA CLIENT + EMBEDDING LOADER (REQUIRED)\n",
    "# ================================================================\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# 1) Define where your Chroma DB will be stored\n",
    "PERSIST_DIR = \"chroma_store\"\n",
    "\n",
    "# 2) Choose a good embedding model\n",
    "# all-MiniLM-L6-v2 = best lightweight CPU embedding model\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "embed_model = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL_NAME\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Function: get chroma client (persistent)\n",
    "# ---------------------------------------------------------------\n",
    "def get_chroma_client():\n",
    "    return chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Function: embed texts safely\n",
    "# ---------------------------------------------------------------\n",
    "def embed_texts(text_list):\n",
    "    return embed_model(text_list)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Function: Upsert PDF into Chroma\n",
    "# (this creates document chunks + embeddings)\n",
    "# ---------------------------------------------------------------\n",
    "def upsert_pdf_to_chroma(pdf_path, collection_name):\n",
    "    client = get_chroma_client()\n",
    "\n",
    "    try:\n",
    "        col = client.get_collection(collection_name)\n",
    "    except:\n",
    "        col = client.create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embed_model\n",
    "        )\n",
    "\n",
    "    # Extract text pages\n",
    "    pages = extract_text_auto(pdf_path)\n",
    "\n",
    "    docs = []\n",
    "    metas = []\n",
    "    ids = []\n",
    "\n",
    "    # Chunk strategy: simple per-page\n",
    "    for i, page in enumerate(pages):\n",
    "        if not page.strip():\n",
    "            continue\n",
    "\n",
    "        chunk_id = f\"{pdf_path}_page_{i}\"\n",
    "        docs.append(page)\n",
    "        metas.append({\"page\": i, \"source\": pdf_path})\n",
    "        ids.append(chunk_id)\n",
    "\n",
    "    # Store in vector DB\n",
    "    col.upsert(\n",
    "        documents=docs,\n",
    "        metadatas=metas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(f\"Vector DB updated → Stored {len(docs)} chunks.\")\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b658aa7b-3576-4872-b0b0-7e362961ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - final combined pipeline (clean + consistent)\n",
    "def process_pdf_hybrid(pdf_path: str, collection_name: str, top_k=5, embed_threshold=0.30):\n",
    "    \n",
    "    # 1) Extract raw text\n",
    "    pages = extract_text_auto(pdf_path)\n",
    "    full_text = \" \".join(pages)\n",
    "    print(f\"Extracted {len(full_text)} characters\")\n",
    "\n",
    "    # 2) Ensure PDF is in Chroma (create if missing)\n",
    "    client = get_chroma_client()\n",
    "    try:\n",
    "        col = client.get_collection(collection_name)\n",
    "    except Exception:\n",
    "        col = upsert_pdf_to_chroma(pdf_path, collection_name)\n",
    "\n",
    "    # 3) Retrieve chunks for fast embedding-based classification\n",
    "    q = full_text[:2000]  \n",
    "    top_chunks = retrieve_similar_chunks(q, collection_name, top_k=top_k)\n",
    "\n",
    "    # 4) CATEGORY detection using keyword scanning\n",
    "    candidates = embedding_candidate_category(top_chunks)\n",
    "    print(\"Embedding candidates:\", candidates)\n",
    "\n",
    "    # 5) Decide category using embeddings OR LLM fallback\n",
    "    if candidates and candidates[0][1] >= embed_threshold:\n",
    "        category = candidates[0][0]\n",
    "        classifier_conf = float(candidates[0][1])\n",
    "        print(f\"Chroma -> category {category} (score {classifier_conf})\")\n",
    "    else:\n",
    "        snippets = [doc for doc, meta, dist in top_chunks if doc]\n",
    "        cls = llm_classify_with_context(\n",
    "            full_text,\n",
    "            model=CLASSIFIER_MODEL,\n",
    "            top_k_snippets=snippets\n",
    "        )\n",
    "        category = cls.get(\"category\", \"Other\")\n",
    "        classifier_conf = float(cls.get(\"confidence\", 0.0))\n",
    "        print(f\"LLM classifier -> category {category} (conf {classifier_conf})\")\n",
    "\n",
    "    # 6) FIELD EXTRACTION using FULL RAG CONTEXT (Option B)\n",
    "    extracted = extract_fields_via_llm(\n",
    "        full_text,\n",
    "        category,\n",
    "        model=EXTRACTION_MODEL,\n",
    "        max_chars=6000,\n",
    "        collection_name=collection_name  # <--- Full RAG added\n",
    "    )\n",
    "    print(\"Extracted keys:\", list(extracted.keys())[:25])\n",
    "\n",
    "    # 7) Confidence scoring\n",
    "    fields_conf = compute_confidence_from_dict(extracted)\n",
    "    combined_conf = round((classifier_conf + fields_conf) / 2, 3)\n",
    "\n",
    "    # 8) Final Output JSON\n",
    "    result = {\n",
    "        \"document_path\": pdf_path,\n",
    "        \"category\": category,\n",
    "        \"classification_confidence\": float(classifier_conf),\n",
    "        \"fields_confidence\": fields_conf,\n",
    "        \"combined_confidence\": combined_conf,\n",
    "        \"extracted\": extracted\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3226395-41ff-4d02-bec4-9437494b5ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: C:/Users/lenovo/Desktop/rag with chatboat/PA_CERTIFICATE_6929363934.pdf\n",
      "Extracted 4813 characters\n",
      "Embedding candidates: []\n",
      "LLM classifier -> category Health Insurance Policy (conf 0.9)\n",
      "Extracted keys: ['Policy Holder Name', 'Policy Number', 'Insurance Company', 'TPA Name', 'Sum Insured', 'Coverage Type', 'Policy Start Date', 'Policy End Date', 'UIN / Product Code', 'AI Summary']\n",
      "{\n",
      "  \"document_path\": \"C:/Users/lenovo/Desktop/rag with chatboat/PA_CERTIFICATE_6929363934.pdf\",\n",
      "  \"category\": \"Health Insurance Policy\",\n",
      "  \"classification_confidence\": 0.9,\n",
      "  \"fields_confidence\": 0.889,\n",
      "  \"combined_confidence\": 0.895,\n",
      "  \"extracted\": {\n",
      "    \"Policy Holder Name\": \"Indian Railway Catering and Tourism Corporation Limited.\",\n",
      "    \"Policy Number\": \"5002004224P101121449000\",\n",
      "    \"Insurance Company\": \"UNITED INDIA INSURANCE COMPANY LIMITED\",\n",
      "    \"TPA Name\": \"Not mentioned\",\n",
      "    \"Sum Insured\": \"10,00,000/- for Death and Permanent Total Disability; upto 7,50,000 for Permanent Partial Disability; Upto 2,00,000/- for Hospitalization Expenses For Injury\",\n",
      "    \"Coverage Type\": \"Travel Insurance Cover for E-Ticket Passengers of IRCTC from Katora to Uslapur on Train Number 18242 (ABKP DURG EXP) on 13/07/2025\",\n",
      "    \"Policy Start Date\": \"13/07/2025 23:35:00\",\n",
      "    \"Policy End Date\": \"14/07/2025 06:35:00\",\n",
      "    \"UIN / Product Code\": \"IRDAN545CP0275V01200708\",\n",
      "    \"AI Summary\": \"This travel insurance policy covers NANDANI SAHU (age 19, female) and VINOD SAHU (age 50, male) for their journey from Katora to Uslapur on Train Number 18242 (ABKP DURG EXP) on 13/07/2025.\"\n",
      "  }\n",
      "}\n",
      "Saved extracted_result.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 - choose file manually or via dialog and run\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "Tk().withdraw()\n",
    "\n",
    "pdf_path = askopenfilename(title=\"Select PDF file\", filetypes=[(\"PDF files\",\"*.pdf\")])\n",
    "print(\"Selected:\", pdf_path)\n",
    "\n",
    "collection_name = \"insurance_docs\"   # vector DB\n",
    "res = process_pdf_hybrid(pdf_path, collection_name=collection_name, top_k=5, embed_threshold=0.35)\n",
    "\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "\n",
    "with open(\"extracted_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(res, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved extracted_result.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "847ce2f2-65c5-4bdd-9241-b809aec8b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Collection is already empty.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=r\"C:\\Users\\lenovo\\Desktop\\rag with chatboat\\chroma_store\"\n",
    ")\n",
    "\n",
    "def reset_collection(collection_name: str):\n",
    "    try:\n",
    "        col = client.get_collection(collection_name)\n",
    "\n",
    "        # get all current items\n",
    "        data = col.get()\n",
    "        ids = data.get(\"ids\", [])\n",
    "\n",
    "        if not ids:\n",
    "            print(\"[INFO] Collection is already empty.\")\n",
    "            return\n",
    "\n",
    "        # delete all by IDs\n",
    "        col.delete(ids=ids)\n",
    "        print(f\"[OK] Deleted {len(ids)} items from collection '{collection_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not reset collection '{collection_name}'. Reason: {e}\")\n",
    "\n",
    "reset_collection(\"insurance_docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce87c7-f4af-45e8-b1fc-f105d9401cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00cff9-e42e-4bfb-a179-74d13019c375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc77a9f-d09a-4a61-89d1-bf9dc23f5a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02dbe9-d31a-47cf-8a7b-18b10d8d8c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818b02c-8368-4a79-9e84-84957638ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a9f57-634f-4e2e-84cb-50f95d7eca20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782ace9-5ba3-450c-b7b6-4dc437c6dc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfd863-b1cf-4b71-97f3-48115b4710e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7429ee6-32fd-4e7f-acb2-4018b5ed0972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Gemini Env)",
   "language": "python",
   "name": "gemini_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
